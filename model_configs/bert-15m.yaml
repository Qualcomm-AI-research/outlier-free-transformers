# Same thing
max_position_embeddings: 256
max_seq_length: 256 # max context length

num_hidden_layers: 6 # n_blocks
num_attention_heads: 6 # n_heads
hidden_size: 288 # dim
intermediate_size: 768 # mlp: round(3*dim, 256)
gradient_checkpointing: False

per_device_train_batch_size: 128
per_device_eval_batch_size: 128
gradient_accumulation_steps: 1

learning_rate: 0.0008
max_train_steps: 1000000
